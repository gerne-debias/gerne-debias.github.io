<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Gradient Extrapolation for Debiased Representation Learning">
  <meta name="keywords" content="GERNE, Gradient Extrapolation, Debiasing, Representation Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-site-verification" content="F-y1W_hFsTfcvIc9VQA0FxQojvOE2ogTVRtqLE1UJZg" />
  <title>GERNE: Gradient Extrapolation for Debiased Representation Learning</title>
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="./static/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.ico" type="image/x-icon">

  <script src="./static/js/jquery-3.5.1.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://scholar.google.com/citations?user=EbOfr_YAAAAJ&hl=en">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Links
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://eifer-mam.github.io/">
              CVPR 2025 - EIFER
            </a>
            <a class="navbar-item" href="https://causalrivers.github.io/">
              ICLR 2025 - CausalRivers
            </a>
            <a class="navbar-item" href="https://fastcav.github.io/">
              ICML 2025 - FastCAV
            </a>
            <hr class="navbar-divider">
            <a class="navbar-item" href="https://github.com/cvjena">
              Computer Vision Group Github
            </a>
            <a class="navbar-item" href="https://inf-cv.uni-jena.de">
              Computer Vision Group Homepage
            </a>
            <hr class="navbar-divider">
            <a class="navbar-item" href="https://www.uni-jena.de/en">
              Friedrich Schiller University Jena
            </a>
          </div>
        </div>
      </div>
    </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="spotbox spotlight">
              <h1 class="title is-2 publication-title">
                <u>GERNE</u><br> Gradient Extrapolation for Debiased Representation Learning
              </h1>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=EbOfr_YAAAAJ&hl=en">Ihab Asaad</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=ElQ7URQAAAAJ&hl=en">Maha Shadaydeh</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=bhpi3vgAAAAJ">Joachim Denzler</a><sup>1</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Computer Vision Group, Friedrich Schiller University Jena, Germany</span>
              <!-- <span class="author-block"><sup>2</sup>Jena University Hospital</span> -->
              <!-- <br>
              <span class="author-block">Friedrich Schiller University Jena, Germany</span> -->
            </div>
            <div class="is-size-4 publication-venue">
              <div style="transform: scaleX(-1); display: inline-block;">ðŸŽ‰</div>
              ICCV 2025
              ðŸŽ‰
            </div>
            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2503.13236" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv Preprint</span>
                  </a>
                </span>
                <span class="link-block">
                  <a class="external-link button is-normal is-rounded is-dark">
                  <!-- <a href="./static/files/EIFER_supp.zip" class="external-link button is-normal is-rounded is-dark"> -->
                    <span class="icon">
                      <i class="far fa-file-archive"></i>
                    </span>
                    <span>Supplementary Material (Soon)</span>
                  </a>
                </span>
                <span class="link-block">
                  <a class="external-link button is-normal is-rounded is-dark">
                  <!-- <a href="https://cloud.uni-jena.de/apps/forms/embed/aoCkxWQit2az3wZqkWgRj882" class="external-link button is-normal is-rounded is-dark"> -->
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Soon)</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
    </div>
    </div>
  </section>

  <!-- <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <div class="content">
            <div class="publication-video">
              <iframe title="Teaser Video" src="./static/videos/TeaserWebsiteC.mp4" frameborder="0" frameborder="0" allow="autoplay; encrypted-media; muted" allowfullscreen width="800"></iframe>
            </div>
            <p style="font-size: 0.95rem;" class="is-centered">
              <strong>Bridging the gap between mimics and muscles:</strong>
              Our method <strong style="color: rgb(83, 83, 207);">EIFER</strong> utilizes neural unpaired image-to-image translation to decouple facial geometry and appearance for muscle-activity-based expression synthesis and electrode-free facial electromyography.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section> -->
  <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <div class="content">
          <div class="publication-video">
            <img src="./static/images/gerne.png" alt="GERNE Image" width="800" />
          </div>
          <p style="font-size: 0.95rem;" class="is-centered">
            <strong>Bridging the gap between mimics and muscles:</strong>
            Our method <strong style="color: rgb(83, 83, 207);">EIFER</strong> utilizes neural unpaired image-to-image translation to decouple facial geometry and appearance for muscle-activity-based expression synthesis and electrode-free facial electromyography.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Machine learning classification models trained with empirical risk minimization (ERM) often inadvertently rely on spurious correlations. When absent in the test data, these unintended associations between non-target attributes and target labels lead to poor generalization.
              This paper addresses this problem from a model optimization perspective and proposes a novel method, <u>G</u>radient <u>E</u>xtrapolation for Debiased <u>R</u>epresentatio<u>n</u> L<u>e</u>arning (GERNE), designed to learn debiased representations in both known and unknown attribute training cases. 
              GERNE uses two distinct batches with different amounts of spurious correlations and defines the target gradient as a linear extrapolation of the gradients computed from each batch's loss. Our analysis shows that when the extrapolated gradient points toward the batch gradient with fewer spurious correlations, it effectively guides training toward learning a debiased model.
              GERNE serves as a general framework for debiasing, encompassing methods such as ERM, reweighting, and resampling, as special cases. 
              We derive the theoretical upper and lower bounds of the extrapolation factor employed by GERNE. By tuning this factor, GERNE can adapt to maximize either Group-Balanced Accuracy (GBA) or Worst-Group Accuracy (WGA). 
              We validate the proposed approach on five vision and one NLP benchmarks, demonstrating competitive and often superior performance compared to state-of-the-art baselines.
              <!-- The code is available at: <a href="https://gerne-debias.github.io/" target="_blank">https://gerne-debias.github.io/</a> -->
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Main Insights</h2>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
            <img src="./static/images/eifer_qualitative.jpg" alt="Geometry Reconstruction Experiments" />
            <div class="caption">
              <strong>Facial Geometry Reconstruction</strong>
              <br>
              Our method <strong>EIFER</strong> faithfully reconstructs the facial geometry under strong sEMG electrode occlusion.
              It correctly captures the expression, identity, and pose of the subject.
              Existing methods struggle to align the face due to already misaligned landmark placements.
              Further, the identity parameters diverge even with the same person, making them unsuitable for our task.
            </div>
          </div>
          <div class="item">
            <img src="./static/images/eifer_qualitative_recon.jpg" alt="Appearance Reconstruction Experiments" />
            <div class="caption">
              <strong>Facial Appearance Reconstruction</strong>
              <br>
              Our method separates the facial appearance from the geometry, allowing for faithful reconstruction of the skin texture, lighting, and occluded electrodes.
              While existing methods inherently restore the occluded regions due to the utilized appearance model, our method creates more photorealistic results due to the adversarial cyclic training.
            </div>
          </div>
          <div class="item">
            <img src="./static/images/eifer_exp2emg.jpg" alt="Electrode-Free Facial EMG" />
            <div class="caption">
              <strong>Electrode-Free Facial EMG</strong>
              <br>
              Our method allows for the prediction of muscle activity based on observed facial expressions, this is a new paradigm for <strong>electrode-free facial electromyography</strong>.
              This enables the reconstruction of muscle activity without the need for electrodes, providing a new way to analyze facial expressions.
              We demonstrate that our method can predict muscle activity from observed expressions and synthesize expressions based on muscle activity.
              We provide next to <strong>EIFER</strong> several <strong>Exp2EMG</strong> models for existing extractors like <strong>DECA</strong>, <strong>EMOCAv2</strong>, <strong>SMIRK</strong>,<strong>Deep3DFace</strong>, and <strong>FOCUS</strong>.
              Hence, both <strong>FLAME</strong> and <strong>BFM</strong> expression spaces are connected to the muscle activity.
            </div>
          </div>
          <div class="item">
            <img src="./static/images/eifer_emg2exp.jpg" alt="Expression Synthesis" />
            <div class="caption">
              <strong>Expression Synthesis</strong>
              <br>
              In the second phase of <strong>EIFER</strong> we learn to synthesize facial expressions based on muscle activity.
              This allows to generate expression based solely on muscle activity and provides a new way to analyze facial expressions, especially interesting for possible advances in camera-free animation captures.
              While <strong>EIFER</strong> creates the most realistic results, we will also publish EMG2Exp of <strong>MC-CycleGAN+Model</strong> combinations.
              Therefore, both the expression space of both <strong>FLAME</strong> and <strong>BFM</strong> are now connected to the muscle activity.
            </div>
          </div>
          <div class="item">
            <img src="./static/images/01_face-at-rest_Geo.jpg" alt="Expression Synthesis" />
            <div class="caption">
              <strong>More Geometry Examples - Fact At Rest</strong>
              <br>
              Our method <strong>EIFER</strong> faithfully reconstructs the facial geometry under strong sEMG electrode occlusion.
              This example shows the face at rest, where the subject is not performing any facial expression.
            </div>
          </div>
          <div class="item">
            <img src="./static/images/02_eye-tight_Geo.jpg" alt="Expression Synthesis" />
            <div class="caption">
              <strong>More Geometry Examples - Eyes closed tightly</strong>
              <br>
              Our method <strong>EIFER</strong> faithfully reconstructs the facial geometry under strong sEMG electrode occlusion.
              This example shows the subject closing their eyes tightly, which is a challenging due to the similarity of a soft eye closure or rapid blinking.
            </div>
          </div>
          <div class="item">
            <img src="./static/images/03_smile-open_Geo.jpg" alt="Expression Synthesis" />
            <div class="caption">
              <strong>More Geometry Examples - Smile Open</strong>
              <br>
              Our method <strong>EIFER</strong> faithfully reconstructs the facial geometry under strong sEMG electrode occlusion.
              This example shows the subject smiling with their mouth open, which is a challenging as the mouth inside should not be visible.
            </div>
          </div>
          <div class="item">
            <img src="./static/images/04_snarl_Geo.jpg" alt="Expression Synthesis" />
            <div class="caption">
              <strong>More Geometry Examples - Snarl</strong>
              <br>
              Our method <strong>EIFER</strong> faithfully reconstructs the facial geometry under strong sEMG electrode occlusion.
              This example shows the subject snarling, which is a challenging expression due to the wrinkles and the mouth opening downwards.
            </div>
          </div>
          <div class="item">
            <img src="./static/images/05_nose_Geo.jpg" alt="Expression Synthesis" />
            <div class="caption">
              <strong>More Geometry Examples - Nose wrinkling</strong>
              <br>
              Our method <strong>EIFER</strong> faithfully reconstructs the facial geometry under strong sEMG electrode occlusion.
              This example shows the subject wrinkling their nose, which is a challenging expression due to the wrinkles only the nose bridge and forehead (glabella) should be affected.
            </div>
          </div>

        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3">Method</h2>
            <div class="content has-text-justified">
              <img src="./static/images/eifer_method.jpg" alt="EIFER Method" class="interpolation-image" width="auto" />
              <p style="line-height: 1.6;">
                <b>EIFER (Electromyography-Informed Facial Expression Reconstruction)</b> introduces a novel approach for facial expression analysis that addresses the challenges posed by surface electromyography (sEMG) electrode occlusion.
                Our method is grounded in the principle of decoupling facial geometry and appearance, achieved through a neural unpaired image-to-image translation framework, see the above figure.
                EIFER leverages <b>3D Morphable Models (3DMMs)</b>, specifically <b>FLAME</b> , to provide a parametric representation of facial shape and expression.
                <u>We see the 3DMM expression space and muscle activity as the missing link between computer animation and physiologically-grounded facial expression analysis.</u>
              </p>
              <p style="line-height: 1.6;">
                For monocular 3D face reconstruction, EIFER employs <b>neural differential rendering</b> and utilizes pre-trained <b>SMIRK</b> encoder networks.
                These encoders, consisting of sub-encoders based on MobileNetV3 backbones, estimate 3DMM parameters (shape, expression, pose) from the sEMG electrode occluded input facial images.
                The core of EIFER's occlusion handling lies in a <b>CycleGAN-like adversarial architecture</b> for unpaired image-to-image translation.
                This architecture comprises generator and discriminator networks trained adversarially using unpaired sets of sEMG-occluded and occlusion-free (reference) facial images.
                Crucially, EIFER learns a <b>bidirectional mapping</b> between the 3DMM expression parameter space and measured sEMG muscle activity through multi-layer perceptron (MLPs).
                This bidirectional mapping enables both the synthesis of facial expressions from muscle activity and the prediction of muscle activity from observed facial expressions, effectively achieving electrode-free facial electromyography.
              </p>
              <h4 class="title">Key Advantages of EIFER:</h4>
              <ul>
                <li>
                  <b>Reconstructs Facial Expressions Under Occlusion:</b>
                  EIFER uniquely addresses the challenge of sensor occlusion, accurately reconstructing facial expressions even when electrodes or other visual obstructions are present.
                </li>
                <li>
                  <b>Combines Video and Muscle Activity Data:</b>
                  By leveraging both video and muscle activity (sEMG) data, EIFER provides a more comprehensive and physiologically grounded fully data-driven understanding of facial expressions.
                </li>
                <li>
                  <b>Enables Electrode-Free Facial Electromyography:</b>
                  EIFER can predict muscle activity from facial expressions alone, paving the way for non-invasive, electrode-free facial electromyography in the future.
                </li>
                <li>
                  <b>Provides Accurate 3D Facial Geometry and Appearance:</b>
                  EIFER not only reconstructs a visually realistic face but also captures the underlying 3D geometry, providing richer data for analysis and synthesis.
                </li>
                <li>
                  <b>Utilizes Advanced AI Techniques:</b>
                  Built upon state-of-the-art 3D Morphable Models and unpaired image-to-image translation, EIFER represents a significant advancement in facial analysis technology.
                </li>
                <li>
                  <b>Opens Doors for Multi-Modal Facial Analysis:</b>
                  EIFER facilitates the integration of various data streams for a more holistic understanding of facial expressions, with potential applications in medicine, psychology, human-computer interaction, and animation.
                </li>
              </ul>
            </div>
          </div>
        </div>
      </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-4">Related Links</h2>
            <div class="content has-text-justified">
              <p>
                EIFER combines many ideas from recent works in computer vision, graphics, and machine learning. Here are some related works that you might find interesting:
              </p>
              <ul>
                <li>
                  <b><a href="https://flame.is.tue.mpg.de/">FLAME</a></b> introduced a 3D Morphable Model that captures facial shape and expression variations.
                </li>
                <li>
                  <b><a href="https://georgeretsi.github.io/smirk/">SMIRK</a></b> replaced the traditional appearance model with a neural renderer and decoupled the facial encoding into three sub-encoders.
                </li>
                <li>
                  <b><a href="https://junyanz.github.io/CycleGAN/">CycleGAN</a></b> and <b><a href="https://dx.doi.org/10.1007/978-3-031-45382-3_22">MC-CycleGAN</a></b> are unpaired image-to-image translation methods that have inspired our adversarial training.
                </li>
                <li>
                  <b><a href="https://deca.is.tue.mpg.de/">DECA</a></b>, <b><a href="https://emoca.is.tue.mpg.de/">EMOCAv2</a></b>, <b><a href="https://georgeretsi.github.io/smirk/">SMIRK</a></b>, <b><a href="https://github.com/sicxu/Deep3DFaceRecon_pytorch">Deep3DFace</a></b>, and <b><a href="https://github.com/unibas-gravis/Occlusion-Robust-MoFA">FOCUS</a></b> are all state-of-the-art methods for monocular 3D face reconstruction and analysis. We provide <b>Exp2EMG</b> and <b>EMG2Exp</b> models
                  for these methods.
                </li>
              </ul>
            </div>
          </div>
        </div>
      </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-4">Acknowledgments</h2>
            <div class="content has-text-justified">
              <p>
                This publication is a joint effort of the <a href="https://inf-cv.uni-jena.de/">Computer Vision Group at Friedrich Schiller University Jena</a>and the <a href="https://www.uniklinikum-jena.de/hno/Klinik+f%C3%BCr+Hals__+Nasen_+und+Ohrenheilkunde-page--p-1.html">Department of Otorhinolaryngology at Jena University Hospital</a>.
                The project <a href="https://gepris.dfg.de/gepris/projekt/427899908"><b>Bridging the gap: Mimics and Muscles</b></a> was funded by the <b><a href="https://www.dfg.de/">German Science Foundation</a></b> under the grant number <a href="https://gepris.dfg.de/gepris/projekt/427899908"><b>DFG DE-735/15-1</b> and <b>DFG GU-463/12-1</b></a>.
                We would like to thank all participants who volunteered for the data collection and the reviewers for their valuable feedback.
              </p>
              <p>
                We thank
                <a href="https://scholar.google.com/citations?user=s9bfMqEAAAAJ">Niklas Penzel</a>,
                <a href="https://scholar.google.com/citations?user=Y4bL1asAAAAJ">Gideon Stein</a>,
                <a href="https://scholar.google.com/citations?user=hoNWqu4AAAAJ">Sai Vemuri</a>,
                <a href="https://scholar.google.com/citations?user=ZWPqDxEAAAAJ">Sven Sickert</a>, and
                <a href="https://scholar.google.com/citations?user=ElQ7URQAAAAJ">Maha Shadaydeh</a>
                for their manuscript feedback and advice throughout the project.
                Additionally, we would like to thank
                Nadiya MÃ¼ller,
                Vanessa Tretzsch,
                Martin Heinrich,
                Anna-Maria Kuttenreich,
                <a href="https://scholar.google.com/citations?user=-gcoOtwAAAAJ">Christian Dobel</a>,
                <a href="https://scholar.google.com/citations?user=qSZrJ84AAAAJ">Gerd Fabian Volk</a>,
                Roland GraÃŸme,
                <a href="https://scholar.google.com/citations?user=j-_oDsUAAAAJ">Paul Funk</a>, and
                Richard Schneider
                for their support.
                Furthermore, we thank <a href="https://eggerbernhard.ch/">Bernhard Egger</a> highlighting the potential of our data for facial expression synthesis.
              </p>
            </div>
          </div>
        </div>
      </div>
  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content is-centered has-text-justified">
      <h2 class="title">BibTeX (arXiv Preprint)</h2>
      <!-- <span>
        If you find our work useful, utilize with our models, start your own research with our data set, or use our parts of our code, please cite our work:
      </span> -->
      <pre><code>@misc{asaad2025gradientextrapolationdebiasedrepresentation,
      title={Gradient Extrapolation for Debiased Representation Learning}, 
      author={Ihab Asaad and Maha Shadaydeh and Joachim Denzler},
      year={2025},
      eprint={2503.13236},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2503.13236}, 
}
</code></pre>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="./static/files/EIFER.pdf">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://timozen.github.io" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
              This page was built using <a href="https://github.com/eifer-mam/eifer-mam.github.io">here</a> which was adopted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> project page.
              If you want to use the design, please follow the original license and link.
            </p>
            <p>
              <a rel="license" href="impressum_privacy.html">Impressum und Datenschutz</a>
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <div id="imageModal" class="modal">
    <span class="close">&times;</span>
    <img class="modal-content" id="modalImage">
  </div>
</body>

</html>