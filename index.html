<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Electromyography-Informed Facial Expression Reconstruction For Physiological-Based Synthesis and Analysis">
  <meta name="keywords" content="EIFER, 3DMM, Monocular 3D Face Reconstruction, Mimics and Muscles">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-site-verification" content="F-y1W_hFsTfcvIc9VQA0FxQojvOE2ogTVRtqLE1UJZg" />
  <title>EIFER: Electromyography-Informed Facial Expression Reconstruction For Physiological-Based Synthesis and Analysis</title>
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="./static/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/eifer.png" type="image/x-icon">

  <script src="./static/js/jquery-3.5.1.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://timozen.github.io">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Links
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://causalrivers.github.io/">
              ICLR 2025 - CausalRivers
            </a>
            <a class="navbar-item" href="https://fastcav.github.io/">
              ICML 2025 - FastCAV
            </a>
            <hr class="navbar-divider">
            <a class="navbar-item" href="https://github.com/cvjena">
              Computer Vision Group Github
            </a>
            <a class="navbar-item" href="https://inf-cv.uni-jena.de">
              Computer Vision Group Homepage
            </a>
            <hr class="navbar-divider">
            <a class="navbar-item" href="https://www.uni-jena.de/en">
              Friedrich Schiller University Jena
            </a>
            <a class="navbar-item" href="https://www.uniklinikum-jena.de/hno/Klinik+f%C3%BCr+Hals__+Nasen_+und+Ohrenheilkunde-page--p-1.html">
              University Hospital Jena - ENT Department
            </a>
          </div>
        </div>
      </div>
    </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="spotbox spotlight">
              <h1 class="title is-2 publication-title">
                <u>EIFER</u><br> Electromyography-Informed Facial Expression Reconstruction For Physiological-Based Synthesis and Analysis
              </h1>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://timozen.github.io">Tim BÃ¼chner</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=Iz6uugoAAAAJ">Christoph Anders</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=sbBPoc0AAAAJ">Orlando Guntinas-Lichius</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=bhpi3vgAAAAJ">Joachim Denzler</a><sup>1</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Computer Vision Group</span>
              <span class="author-block"><sup>2</sup>Jena University Hospital</span>
              <br>
              <span class="author-block">Friedrich Schiller University Jena, Germany</span>
            </div>
            <div class="is-size-4 publication-venue">
              <div style="transform: scaleX(-1); display: inline-block;">ðŸŽ‰</div>
              CVPR 2025 Highlight
              ðŸŽ‰
            </div>
            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2503.09556" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv Preprint</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="./static/files/EIFER_supp.zip" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-file-archive"></i>
                    </span>
                    <span>Supplementary Material</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://cloud.uni-jena.de/apps/forms/embed/aoCkxWQit2az3wZqkWgRj882" class="external-link button is-normal is-rounded is-dark animated-gradient">
                    <span class="icon">
                      <i class="fas fa-info"></i>
                    </span>
                    <span>Get Updates</span>
                  </a>
                </span>
                <br>
                <span class="link-block">
                  <a href="https://cloud.uni-jena.de/apps/forms/embed/aoCkxWQit2az3wZqkWgRj882" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Soon)</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://cloud.uni-jena.de/apps/forms/embed/aoCkxWQit2az3wZqkWgRj882" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Models (Soon)</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://cloud.uni-jena.de/apps/forms/embed/aoCkxWQit2az3wZqkWgRj882" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Data Sample (Soon)</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://cloud.uni-jena.de/apps/forms/embed/aoCkxWQit2az3wZqkWgRj882" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Data Set (Soon)</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
    </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <div class="content">
            <div class="publication-video">
              <iframe title="Teaser Video" src="./static/videos/TeaserWebsiteC.mp4" frameborder="0" frameborder="0" allow="autoplay; encrypted-media; muted" allowfullscreen width="800"></iframe>
            </div>
            <p style="font-size: 0.95rem;" class="is-centered">
              <strong>Bridging the gap between mimics and muscles:</strong>
              Our method <strong style="color: rgb(83, 83, 207);">EIFER</strong> utilizes neural unpaired image-to-image translation to decouple facial geometry and appearance for muscle-activity-based expression synthesis and electrode-free facial electromyography.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            The relationship between muscle activity and resulting facial expressions is crucial for various fields, including psychology, medicine, and entertainment.
            The synchronous recording of facial mimicry and muscular activity via surface electromyography (sEMG) provides a unique window into these complex dynamics.
            Unfortunately, existing methods for facial analysis cannot handle electrode occlusion, rendering them ineffective.
            Even with occlusion-free reference images of the same person, variations in expression intensity and execution are unmatchable.
            Our electromyography-informed facial expression reconstruction (EIFER) approach is a novel method to restore faces under sEMG occlusion faithfully in an adversarial manner.
            We decouple facial geometry and visual appearance (e.g., skin texture, lighting, electrodes) by combining a 3D Morphable Model (3DMM) with neural unpaired image-to-image translation via reference recordings.
            Then, EIFER learns a bidirectional mapping between 3DMM expression parameters and muscle activity, establishing correspondence between the two domains.
            We validate the effectiveness of our approach through experiments on a dataset of synchronized sEMG recordings and facial mimicry, demonstrating faithful geometry and appearance reconstruction.
            Further, we synthesize expressions based on muscle activity and how observed expressions can predict dynamic muscle activity.
            Consequently, EIFER introduces a new paradigm for facial electromyography, which could be extended to other forms of multi-modal face recordings.
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Main Insights</h2>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
            <img src="./static/images/eifer_qualitative.jpg" alt="Geometry Reconstruction Experiments" />
            <div class="caption">
              <strong>Facial Geometry Reconstruction</strong>
              <br>
              Our method <strong>EIFER</strong> faithfully reconstructs the facial geometry under strong sEMG electrode occlusion.
              It correctly captures the expression, identity, and pose of the subject.
              Existing methods struggle to align the face due to already misaligned landmark placements.
              Further, the identity parameters diverge even with the same person, making them unsuitable for our task.
            </div>
          </div>
          <div class="item">
            <img src="./static/images/eifer_qualitative_recon.jpg" alt="Appearance Reconstruction Experiments" />
            <div class="caption">
              <strong>Facial Appearance Reconstruction</strong>
              <br>
              Our method separates the facial appearance from the geometry, allowing for faithful reconstruction of the skin texture, lighting, and occluded electrodes.
              While existing methods inherently restore the occluded regions due to the utilized appearance model, our method creates more photorealistic results due to the adversarial cyclic training.
            </div>
          </div>
          <div class="item">
            <img src="./static/images/eifer_exp2emg.jpg" alt="Electrode-Free Facial EMG" />
            <div class="caption">
              <strong>Electrode-Free Facial EMG</strong>
              <br>
              Our method allows for the prediction of muscle activity based on observed facial expressions, this is a new paradigm for <strong>electrode-free facial electromyography</strong>.
              This enables the reconstruction of muscle activity without the need for electrodes, providing a new way to analyze facial expressions.
              We demonstrate that our method can predict muscle activity from observed expressions and synthesize expressions based on muscle activity.
              We provide next to <strong>EIFER</strong> several <strong>Exp2EMG</strong> models for existing extractors like <strong>DECA</strong>, <strong>EMOCAv2</strong>, <strong>SMIRK</strong>,<strong>Deep3DFace</strong>, and <strong>FOCUS</strong>.
              Hence, both <strong>FLAME</strong> and <strong>BFM</strong> expression spaces are connected to the muscle activity.
            </div>
          </div>
          <div class="item">
            <img src="./static/images/eifer_emg2exp.jpg" alt="Expression Synthesis" />
            <div class="caption">
              <strong>Expression Synthesis</strong>
              <br>
              In the second phase of <strong>EIFER</strong> we learn to synthesize facial expressions based on muscle activity.
              This allows to generate expression based solely on muscle activity and provides a new way to analyze facial expressions, especially interesting for possible advances in camera-free animation captures.
              While <strong>EIFER</strong> creates the most realistic results, we will also publish EMG2Exp of <strong>MC-CycleGAN+Model</strong> combinations.
              Therefore, both the expression space of both <strong>FLAME</strong> and <strong>BFM</strong> are now connected to the muscle activity.
            </div>
          </div>
          <div class="item">
            <img src="./static/images/01_face-at-rest_Geo.jpg" alt="Expression Synthesis" />
            <div class="caption">
              <strong>More Geometry Examples - Fact At Rest</strong>
              <br>
              Our method <strong>EIFER</strong> faithfully reconstructs the facial geometry under strong sEMG electrode occlusion.
              This example shows the face at rest, where the subject is not performing any facial expression.
            </div>
          </div>
          <div class="item">
            <img src="./static/images/02_eye-tight_Geo.jpg" alt="Expression Synthesis" />
            <div class="caption">
              <strong>More Geometry Examples - Eyes closed tightly</strong>
              <br>
              Our method <strong>EIFER</strong> faithfully reconstructs the facial geometry under strong sEMG electrode occlusion.
              This example shows the subject closing their eyes tightly, which is a challenging due to the similarity of a soft eye closure or rapid blinking.
            </div>
          </div>
          <div class="item">
            <img src="./static/images/03_smile-open_Geo.jpg" alt="Expression Synthesis" />
            <div class="caption">
              <strong>More Geometry Examples - Smile Open</strong>
              <br>
              Our method <strong>EIFER</strong> faithfully reconstructs the facial geometry under strong sEMG electrode occlusion.
              This example shows the subject smiling with their mouth open, which is a challenging as the mouth inside should not be visible.
            </div>
          </div>
          <div class="item">
            <img src="./static/images/04_snarl_Geo.jpg" alt="Expression Synthesis" />
            <div class="caption">
              <strong>More Geometry Examples - Snarl</strong>
              <br>
              Our method <strong>EIFER</strong> faithfully reconstructs the facial geometry under strong sEMG electrode occlusion.
              This example shows the subject snarling, which is a challenging expression due to the wrinkles and the mouth opening downwards.
            </div>
          </div>
          <div class="item">
            <img src="./static/images/05_nose_Geo.jpg" alt="Expression Synthesis" />
            <div class="caption">
              <strong>More Geometry Examples - Nose wrinkling</strong>
              <br>
              Our method <strong>EIFER</strong> faithfully reconstructs the facial geometry under strong sEMG electrode occlusion.
              This example shows the subject wrinkling their nose, which is a challenging expression due to the wrinkles only the nose bridge and forehead (glabella) should be affected.
            </div>
          </div>

        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3">Method</h2>
            <div class="content has-text-justified">
              <img src="./static/images/eifer_method.jpg" alt="EIFER Method" class="interpolation-image" width="auto" />
              <p style="line-height: 1.6;">
                <b>EIFER (Electromyography-Informed Facial Expression Reconstruction)</b> introduces a novel approach for facial expression analysis that addresses the challenges posed by surface electromyography (sEMG) electrode occlusion.
                Our method is grounded in the principle of decoupling facial geometry and appearance, achieved through a neural unpaired image-to-image translation framework, see the above figure.
                EIFER leverages <b>3D Morphable Models (3DMMs)</b>, specifically <b>FLAME</b> , to provide a parametric representation of facial shape and expression.
                <u>We see the 3DMM expression space and muscle activity as the missing link between computer animation and physiologically-grounded facial expression analysis.</u>
              </p>
              <p style="line-height: 1.6;">
                For monocular 3D face reconstruction, EIFER employs <b>neural differential rendering</b> and utilizes pre-trained <b>SMIRK</b> encoder networks.
                These encoders, consisting of sub-encoders based on MobileNetV3 backbones, estimate 3DMM parameters (shape, expression, pose) from the sEMG electrode occluded input facial images.
                The core of EIFER's occlusion handling lies in a <b>CycleGAN-like adversarial architecture</b> for unpaired image-to-image translation.
                This architecture comprises generator and discriminator networks trained adversarially using unpaired sets of sEMG-occluded and occlusion-free (reference) facial images.
                Crucially, EIFER learns a <b>bidirectional mapping</b> between the 3DMM expression parameter space and measured sEMG muscle activity through multi-layer perceptron (MLPs).
                This bidirectional mapping enables both the synthesis of facial expressions from muscle activity and the prediction of muscle activity from observed facial expressions, effectively achieving electrode-free facial electromyography.
              </p>
              <h4 class="title">Key Advantages of EIFER:</h4>
              <ul>
                <li>
                  <b>Reconstructs Facial Expressions Under Occlusion:</b>
                  EIFER uniquely addresses the challenge of sensor occlusion, accurately reconstructing facial expressions even when electrodes or other visual obstructions are present.
                </li>
                <li>
                  <b>Combines Video and Muscle Activity Data:</b>
                  By leveraging both video and muscle activity (sEMG) data, EIFER provides a more comprehensive and physiologically grounded fully data-driven understanding of facial expressions.
                </li>
                <li>
                  <b>Enables Electrode-Free Facial Electromyography:</b>
                  EIFER can predict muscle activity from facial expressions alone, paving the way for non-invasive, electrode-free facial electromyography in the future.
                </li>
                <li>
                  <b>Provides Accurate 3D Facial Geometry and Appearance:</b>
                  EIFER not only reconstructs a visually realistic face but also captures the underlying 3D geometry, providing richer data for analysis and synthesis.
                </li>
                <li>
                  <b>Utilizes Advanced AI Techniques:</b>
                  Built upon state-of-the-art 3D Morphable Models and unpaired image-to-image translation, EIFER represents a significant advancement in facial analysis technology.
                </li>
                <li>
                  <b>Opens Doors for Multi-Modal Facial Analysis:</b>
                  EIFER facilitates the integration of various data streams for a more holistic understanding of facial expressions, with potential applications in medicine, psychology, human-computer interaction, and animation.
                </li>
              </ul>
            </div>
          </div>
        </div>
      </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-4">Related Links</h2>
            <div class="content has-text-justified">
              <p>
                EIFER combines many ideas from recent works in computer vision, graphics, and machine learning. Here are some related works that you might find interesting:
              </p>
              <ul>
                <li>
                  <b><a href="https://flame.is.tue.mpg.de/">FLAME</a></b> introduced a 3D Morphable Model that captures facial shape and expression variations.
                </li>
                <li>
                  <b><a href="https://georgeretsi.github.io/smirk/">SMIRK</a></b> replaced the traditional appearance model with a neural renderer and decoupled the facial encoding into three sub-encoders.
                </li>
                <li>
                  <b><a href="https://junyanz.github.io/CycleGAN/">CycleGAN</a></b> and <b><a href="https://dx.doi.org/10.1007/978-3-031-45382-3_22">MC-CycleGAN</a></b> are unpaired image-to-image translation methods that have inspired our adversarial training.
                </li>
                <li>
                  <b><a href="https://deca.is.tue.mpg.de/">DECA</a></b>, <b><a href="https://emoca.is.tue.mpg.de/">EMOCAv2</a></b>, <b><a href="https://georgeretsi.github.io/smirk/">SMIRK</a></b>, <b><a href="https://github.com/sicxu/Deep3DFaceRecon_pytorch">Deep3DFace</a></b>, and <b><a href="https://github.com/unibas-gravis/Occlusion-Robust-MoFA">FOCUS</a></b> are all state-of-the-art methods for monocular 3D face reconstruction and analysis. We provide <b>Exp2EMG</b> and <b>EMG2Exp</b> models
                  for these methods.
                </li>
              </ul>
            </div>
          </div>
        </div>
      </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-4">Acknowledgments</h2>
            <div class="content has-text-justified">
              <p>
                This publication is a joint effort of the <a href="https://inf-cv.uni-jena.de/">Computer Vision Group at Friedrich Schiller University Jena</a>and the <a href="https://www.uniklinikum-jena.de/hno/Klinik+f%C3%BCr+Hals__+Nasen_+und+Ohrenheilkunde-page--p-1.html">Department of Otorhinolaryngology at Jena University Hospital</a>.
                The project <a href="https://gepris.dfg.de/gepris/projekt/427899908"><b>Bridging the gap: Mimics and Muscles</b></a> was funded by the <b><a href="https://www.dfg.de/">German Science Foundation</a></b> under the grant number <a href="https://gepris.dfg.de/gepris/projekt/427899908"><b>DFG DE-735/15-1</b> and <b>DFG GU-463/12-1</b></a>.
                We would like to thank all participants who volunteered for the data collection and the reviewers for their valuable feedback.
              </p>
              <p>
                We thank
                <a href="https://scholar.google.com/citations?user=s9bfMqEAAAAJ">Niklas Penzel</a>,
                <a href="https://scholar.google.com/citations?user=Y4bL1asAAAAJ">Gideon Stein</a>,
                <a href="https://scholar.google.com/citations?user=hoNWqu4AAAAJ">Sai Vemuri</a>,
                <a href="https://scholar.google.com/citations?user=ZWPqDxEAAAAJ">Sven Sickert</a>, and
                <a href="https://scholar.google.com/citations?user=ElQ7URQAAAAJ">Maha Shadaydeh</a>
                for their manuscript feedback and advice throughout the project.
                Additionally, we would like to thank
                Nadiya MÃ¼ller,
                Vanessa Tretzsch,
                Martin Heinrich,
                Anna-Maria Kuttenreich,
                <a href="https://scholar.google.com/citations?user=-gcoOtwAAAAJ">Christian Dobel</a>,
                <a href="https://scholar.google.com/citations?user=qSZrJ84AAAAJ">Gerd Fabian Volk</a>,
                Roland GraÃŸme,
                <a href="https://scholar.google.com/citations?user=j-_oDsUAAAAJ">Paul Funk</a>, and
                Richard Schneider
                for their support.
                Furthermore, we thank <a href="https://eggerbernhard.ch/">Bernhard Egger</a> highlighting the potential of our data for facial expression synthesis.
              </p>
            </div>
          </div>
        </div>
      </div>
  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content is-centered has-text-justified">
      <h2 class="title">BibTeX (arXiv Preprint)</h2>
      <span>
        If you find our work useful, utilize with our models, start your own research with our data set, or use our parts of our code, please cite our work:
      </span>
      <pre><code>@article{buchner2025electromyography,
      title={Electromyography-Informed Facial Expression Reconstruction for Physiological-Based Synthesis and Analysis},
      author={B{\"u}chner, Tim and Anders, Christoph and Guntinas-Lichius, Orlando and Denzler, Joachim},
      journal={arXiv preprint arXiv:2503.09556},
      year={2025}
}
</code></pre>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="./static/files/EIFER.pdf">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://timozen.github.io" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
              The original design was borrowed from <a href="https://github.com/nerfies/nerfies.github.io">here</a>.
              If you want to use the design, please follow the original license and link.
            </p>
            <p>
              <a rel="license" href="impressum_privacy.html">Impressum und Datenschutz</a>
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <div id="imageModal" class="modal">
    <span class="close">&times;</span>
    <img class="modal-content" id="modalImage">
  </div>
</body>

</html>